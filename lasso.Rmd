---
title: "LASSO: Least Absolute Shrinkage and Selection Operator"
author: "Wu Gong"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: false
    code_folding: hide
    toc_depth: 6
    lightbox: true
    gallery: false
    highlight: monochrome
    css: Wu.css
---


	
```{r setup, echo=FALSE, cache=FALSE,warning=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(Wu)
opts_chunk$set(echo=TRUE
             , cache=FALSE
             , eval=TRUE
             , prompt=FALSE
             , results="asis"
             , tidy=FALSE
             , comment=NA
             , message=FALSE
             , warning=FALSE
             , out.width = '80%'
             , class.source='klippy'
               )
eval_fast <- TRUE
eval_slow <- FALSE
klippy::klippy(position = c('top', 'left')
             , tooltip_message = 'Click to copy'
             , tooltip_success = 'Done')
```




# [Back to Index](index.html)


# Simulate data


```{r}
library(data.table)

set.seed(123456)
n <- 1000
dt <- data.table(
    p0 = rep(0.2, n)
  , or1 = rep(1, n)
  , var1 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.3, 0.7))
  , or2 = rep(1.1, n)
  , var2 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.4, 0.6))
  , or3 = rep(1.2, n)
  , var3 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.2, 0.8))
  , or4 = rep(1.5, n)
  , var4 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.3, 0.7))
  , or5 = rep(1.7, n)
  , var5 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
  , or6 = rep(2, n)
  , var6 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.4, 0.6))
  , or7 = rep(5, n)
  , var7 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.1, 0.9))
)


dt <- dt[, odds0 := p0 / (1 - p0)
         ][, log_odds := log(odds0) +
                 var1 * log(or1) +
                 var2 * log(or2) +
                 var3 * log(or3) +
                 var4 * log(or4) +
                 var5 * log(or5) +
                 var6 * log(or6) +
                 var7 * log(or7)
           ][, p := exp(log_odds)/ (1 + exp(log_odds))]

vsample <- function(p){
    sample(c(1, 0), size = 1, replace = TRUE, prob = c(p, 1 - p))
}
vsample <- Vectorize(vsample)

dt <- dt[, outcome := vsample(p)]

unique(dt[, .(or1, or2, or3, or4, or5, or6, or7)]) %>% prt(caption = "Variables with Odds Ratios")

```


# GLM logistic regression on aggregated data

 * The cbind method formula gives glm the numbers in each categories

```{r}
dt1 <- data.table(
    disease = c(55,42)
  , healthy = c(67,34)
  , treatment = c(1,0)
)


m1 <- glm(cbind(disease,healthy) ~ treatment
        , family = binomial, data = dt1)



library(sjPlot)
tab_model(m1)

dt2 <- data.table(
    disease = c(1, 1, 0, 0)
  , treatment = c(1, 0, 1, 0)
  , weight = c(55, 42, 67, 34)
)

m2 <- glm(disease ~ treatment
        , family = binomial
        , data = dt2
        , weights = weight)

library(sjPlot)
tab_model(m2)

```

# GLM

```{r}
m <- glm(outcome ~ var1 + var2 + var3 + var4 + var5 + var6 + var7
       , data = dt
       , family = binomial
         )

library(sjPlot)

tab_model(m)

```

# glmnet

 * $min_{\beta_0, \beta} \frac{1}{N} \sum_{i=1}^N w_il(y_i, \beta_0 + \beta^Tx_i) + \lambda[(1 - \alpha)||\beta||_2^2 + \alpha||\beta||_1]$
 * L1 penalty: $\alpha = 1$

```{r}

mx <- dt[, .(var1, var2, var3, var4, var5, var6, var7)]
mx <- as.matrix(mx)
## ss <- scale(mmx)
## scale_scale <- attr(ss, 'scaled:scale') 

set.seed(123456)
library(glmnet)
lss <- glmnet(x = mx
            , y = dt$outcome
            , family = "binomial"
            , alpha = 1
            , standardize = TRUE
              )

library(plotmo)

plot_glmnet(lss, xvar = "rlambda")

```

# Cross Validation

```{r}
t1 <- Sys.time()
library(doParallel)
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
set.seed(123456)
cvfit <- cv.glmnet(x = mx
                 , y = dt$outcome
                 , type.measure = "auc"
                 , family = "binomial"
                 , standardize = TRUE
                 , parallel = TRUE
                   )
stopCluster(cl)
t2 <- Sys.time()
print(t2 - t1)

plot(cvfit)

```

# The Selected Model

```{r}
glmcoef <- coef(lss, cvfit$lambda.1se)
betas <- data.table(Variable = glmcoef@Dimnames[[1]]
                   , beta = as.matrix(glmcoef)
                    )
colnames(betas) <- c("Variable", "beta")
betas <- betas[, OR := exp(beta)]


 
betas %>% prt(digits = c(0, 6, 6, 6), caption = "Beta Coefficients of the LASSO Selected Model")

```


# AUC of The Selected Model

 * For plotly output to set chunk option

````
 out.height='200%'
````


```{r,  out.height='200%'}
pred <- predict(lss
              , newx = mx
              , type = "response"
              , s = cvfit$lambda.1se
                )
library(pROC)
r <- roc(dt$outcome, pred, ci = TRUE, direction = "<")
plt_roc(r) %>% ann("LASSO AUC on the Dataset")

```


# Adaptive LASSO

```{r}

set.seed(123456)
library(glmnet)
ridge <- glmnet(x = mx
              , y = dt$outcome
              , family = "binomial"
              , alpha = 0
              , standardize = TRUE
                )

ridge_cv <- cv.glmnet(x = mx
                    , y = dt$outcome
                    , alpha = 0
                    , type.measure = "auc"
                    , family = "binomial"
                    , standardize = TRUE
                      )
best_ridge_coef <- as.numeric(coef(ridge, s = ridge_cv$lambda.1se))[-1]


t1 <- Sys.time()
library(doParallel)
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
set.seed(20210817)
cvfit <- cv.glmnet(x = mx
                 , y = dt$outcome
                 , alpha = 1
                 , penalty.factor = 1 / abs(best_ridge_coef)
                 , type.measure = "auc"
                 , family = "binomial"
                 , standardize=TRUE
                 , parallel = TRUE
                   )
stopCluster(cl)
t2 <- Sys.time()
## print(t2 - t1)

lss_adp <- glmnet(x = mx
                , y = dt$outcome
                , alpha = 1
                , penalty.factor = 1 / abs(best_ridge_coef)
                , family = "binomial"
                , standardize = TRUE
                  )


cfs <- lapply(cvfit$lambda, function(x){
  l <- coef(lss_adp, x)
  invisible(l@Dimnames[[1]][l@i + 1])
})

ls <- do.call(c, cfs)
ls <- unique(ls, fromLast = FALSE)[-1]



ls %>% prt(caption = "Order of Variables Selected", col.name = "Variable Name")


glmcoef <- coef(lss_adp, cvfit$lambda.1se)
betas <- data.table(Variable = glmcoef@Dimnames[[1]]
                   , beta = as.matrix(glmcoef)
                    )
colnames(betas) <- c("Variable", "beta")
betas <- betas[, OR := exp(beta)]


 
betas %>% prt(digits = c(0, 6, 6, 6), caption = "Beta Coefficients from Adaptive LASSO")

```



# Mixed-Effect Model

 * Ref: https://davidabugaber.com/blog/f/find-the-optimal-mixed-model-for-your-data-with-glmmlasso
 



## Cross-Validation with glmnet

```{r}

library(glmmLasso)
data(knee)
kn <- as.data.table(knee)
kn <- kn[, age_log2 := log2(age)
         ][, pain.start_log2 := log2(pain.start)
           ][, time_log2 := log2(time)]

mx <- kn[, .(time, age, sex, pain.start, age_log2, pain.start_log2, time_log2)]
mx <- as.matrix(mx)

t1 <- Sys.time()
library(doParallel)
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
set.seed(123456)
cvfit <- cv.glmnet(x = mx
                 , y = kn$pain
                 , lambda = seq(0.001, 1, length = 1000)
                 , gamma = 1
                 , nfolds = 5
                 , keep = TRUE
                 , type.measure = "mse"
                 , standardize = TRUE
                 , parallel = TRUE
                   )
stopCluster(cl)
t2 <- Sys.time()
print(t2 - t1)



bts <- as.data.table(t(as.matrix(cvfit$glmnet.fit$beta)))
bts$lambda <- cvfit$glmnet.fit$lambda

vars <- names(bts)
vars <- vars[!(vars %in% c("lambda"))]

btw <- melt(bts
          , id.vars="lambda"
          , measure.vars = vars)


ggplot(data = btw, aes(x = lambda, y = value, group = variable, color = variable)) + geom_line() + scale_x_continuous(trans = 'log', breaks = c(0.001, 0.002, 0.005, 0.01, 0.1, 1)) +
    coord_cartesian(xlim = c(0.001, 1))


rst <- data.table(lambda = cvfit$lambda
                , cvm = cvfit$cvm
                , cvsd = cvfit$cvsd
                , cvup = cvfit$cvup
                , cvlo = cvfit$cvlo
                  )

varsr <- names(rst)
varsr <- varsr[!(varsr %in% c("lambda"))]

rstw <- melt(rst
          , id.vars="lambda"
          , measure.vars = varsr)

ggplot(data = rst, aes(x = lambda, y = cvm)) + geom_line() + scale_x_continuous(trans = "log") +
    coord_cartesian(xlim = c(0.0001, 1)) 


plot(cvfit)
varImp(Lasso)

as.numeric(Lasso$bestTune[2])

bts[lambda == as.numeric(Lasso$bestTune[2]), ]

plot(cvfit)

str(cvfit)

```


## Five-Fold Cross-Validation LASSO

```{r}

library(glmmLasso)
data(knee)
kn <- as.data.table(knee)
kn <- kn[, age_log2 := log2(age)
         ][, pain.start_log2 := log2(pain.start)
           ][, time_log2 := log2(time)]


m1 <- lm(pain ~ time + age + sex + th + pain.start, data = kn)

library(caret)

train.control.CV5 <- trainControl(method = "cv", number = 10)

Lambda.Range = seq(0.0001, 1.2, length=10000)


Lasso <- train(pain ~ time + age + sex + th + pain.start + time_log2 + age_log2 + pain.start_log2
             , data = kn
             , method = "glmnet"
             , metric="RMSE"
             , trControl = train.control.CV5
             , lambda = Lambda.Range
             , preProcess = c("center", "scale")
             , tuneGrid = expand.grid(alpha = 1, lambda = Lambda.Range)
               )

str(Lasso$bestTune)
str(Lasso$finalModel)


bts <- as.data.table(as.matrix(t(Lasso$finalModel$beta)))
bts$lambda <- rev(Lasso$results$lambda)

vars <- names(bts)
vars <- vars[!(vars %in% c("lambda"))]

btw <- melt(bts
          , id.vars="lambda"
          , measure.vars = vars)


ggplot(data = btw, aes(x = lambda, y = value, group = variable, color = variable)) + geom_line() + scale_y_continuous() +
    coord_cartesian(xlim = c(0, 0.3))


rst <- as.data.table(Lasso$results)[, alpha := NULL]
varsr <- names(rst)
varsr <- varsr[!(varsr %in% c("lambda"))]

rstw <- melt(rst
          , id.vars="lambda"
          , measure.vars = varsr)

ggplot(data = rstw, aes(x = lambda, y = value, group = variable, color = variable)) + geom_line() + scale_y_continuous() +
    coord_cartesian(xlim = c(0, 0.02)) 

varImp(Lasso)

as.numeric(Lasso$bestTune[2])

bts[lambda == as.numeric(Lasso$bestTune[2]), ]

```

## glmmlasso

```{r}
library(glmmLasso)
data(knee)
kn <- as.data.table(knee)
kn <- kn[, age_log2 := log2(age)
         ][, pain.start_log2 := log2(pain.start)
           ][, time_log2 := log2(time)]

mm <- glmmLasso(pain ~ time + age + sex + th + pain.start + time_log2 + age_log2 + pain.start_log2
                , rnd = list(id = ~ 1)
                , family = gaussian()
                , data = kn
                , lambda = 0.001
                , control=list(print.iter=FALSE))

lambda_range <- exp(seq(log(0.0001), log(1), length = 100))

```

# R sessionInfo

```{r}
sessionInfo()

```
