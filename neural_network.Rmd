---
title: "Style"
author: "Wu Gong"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: false
    code_folding: hide
    toc_depth: 6
    lightbox: true
    gallery: false
    highlight: monochrome
    css: Wu.css
---



```{r setup, echo=FALSE, cache=FALSE,warning=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(Wu)
opts_chunk$set(echo=TRUE,
               cache=FALSE,
               eval=TRUE,
               prompt=FALSE,
               results="asis",
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               out.width = '80%')
eval_fast <- TRUE
eval_slow <- FALSE
```



# Index

[Index](index.html)


# Some Concepts

## Cross Entropy


https://machinelearningmastery.com/cross-entropy-for-machine-learning/

 * Information: it quantifies the number of bits required to encode and transmit an event. Lower probability events have more information, higher have less. Skewed probability distribution has low entropy (unsurprising, predictable), and balanced probability distribution (surprising) has high entropy.
 * Calculation of information: h(x) = -log(P(x))
 * Entropy: is the number of bits required to transmit a randomly selected event from a probability distribution.
 * Calculation of Entropy: H(X) = - sum(P(x) * log(P(x))
 * Cross-entropy: the average number of bits needed to encode data coming from one distribution p when we use model q
 * Calculation of Entropy: H(P,Q) = - sum(P(x) * log(Q(x)); log is the base-2 logarithm, meaning that the results are in bits. If base-e, then the units called nats. P may be the target distribution and Q is the approximation of the target dsitribution. The result is a positive number measured in bits and will be equal to the entropy of the distribution if the two probability distributions are identical.
 * Cross-entropy is same as negative log-likelihood for logistic regression for Bernoulli distribution.
 * Cross-entropy is same as the maximum likelihood estimation framework assuming Gaussian continuous probability distribution and minimizing mean squared error.

```{r}
events = c('red', 'green', 'blue')
p = c(0.10, 0.40, 0.50)
q = c(0.80, 0.15, 0.05)

cross_entropy <- function(p, q){
    -sum(p * log2(q))
}

cross_entropy(p, q)
cross_entropy(p, p)
cross_entropy(q, p)

```


# Recurrent Neural Network


## A Basic Recurrent Neural Network

This is a reading note for
https://cran.r-project.org/web/packages/rnn/vignettes/basic_rnn.html

https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks



### Convert integer to bits

```{r}

intToBits(1)
intToBits(2)
intToBits(3)[1:4]

i2b <- function(integer, length=8)
  as.numeric(intToBits(integer))[1:length]

int2bin <- function(integer, length=8)
  t(sapply(integer, i2b, length=length))

```

### Generate Data

```{r}
training_data_size = 20000

# create sample inputs
X1 = sample(0:127, training_data_size, replace=TRUE)
X2 = sample(0:127, training_data_size, replace=TRUE)

# create sample output
Y <- X1 + X2

# convert to binary
X1 <- int2bin(X1)
X2 <- int2bin(X2)
Y  <- int2bin(Y)

class(X1)
dim(X1)


# create 3d array: dim 1: samples; dim 2: time; dim 3: variables
X <- array(c(X1,X2), dim=c(dim(X1),2))
class(X)
dim(X)

```

### Sigmoid function and its derivative function

 * Sigmoid function is a S-shape curve or sigmoid curve. Logistic function is an example of sigmoid function.
 * Derivative of sigmoid is a function of derivative of x.

```{r}

sigmoid <- function(x)
    1 / (1+exp(-x))

sig_to_der <- function(x)
    x*(1-x)

```

### Initial Weights

 * Input has two dimensions (two nodes or two predictors), X1 and X2
 * The hidder layer has six nodes
 * Output has only one node (corresponding to outcome y)

```{r}
binary_dim = 8
alpha      = 0.5
input_dim  = 2
hidden_dim = 6
output_dim = 1

# initialize weights randomly between -1 and 1, with mean 0
weights_0 = matrix(runif(n = input_dim *hidden_dim, min=-1, max=1),
                   nrow=input_dim,
                   ncol=hidden_dim )
weights_h = matrix(runif(n = hidden_dim*hidden_dim, min=-1, max=1),
                   nrow=hidden_dim,
                   ncol=hidden_dim )
weights_1 = matrix(runif(n = hidden_dim*output_dim, min=-1, max=1),
                   nrow=hidden_dim,
                   ncol=output_dim )

# create matrices to store updates, to be used in backpropagation
weights_0_update = matrix(0, nrow = input_dim,  ncol = hidden_dim)
weights_h_update = matrix(0, nrow = hidden_dim, ncol = hidden_dim)
weights_1_update = matrix(0, nrow = hidden_dim, ncol = output_dim)

```


### Matrix Operation

```{r}
dim(X)
dim(weights_0)

t1 <- cbind(X1[3, 6], X2[3, 6])
dim(t1)
t2 <- t1 %*% weights_0
dim(t2)

dim(weights_1)
t3 <- t2 %*% weights_1
dim(t3)
print(t3)
print(sigmoid(t3))


```



### Training Algorithmn

```{r}
# training logic
for (j in 1:training_data_size) {
    # select data
    a = X1[j,]
    b = X2[j,]

    # select true answer
    c = Y[j,]

    # where we'll store our best guesss (binary encoded)
    d = matrix(0, nrow = 1, ncol = binary_dim)

    overallError = 0

    layer_2_deltas = matrix(0)
    layer_1_values = matrix(0, nrow=1, ncol = hidden_dim)

    # moving along the positions in the binary encoding
    for (position in 1:binary_dim) {
        # generate input and output
        X = cbind( a[position], b[position] ) # rename X to layer_0?
        y = c[position]

        # hidden layer
        layer_1 = sigmoid( (X%*%weights_0) +
                    (layer_1_values[dim(layer_1_values)[1],] %*% weights_h) )

        # output layer
        layer_2 = sigmoid(layer_1 %*% weights_1)

        # did we miss?... if so, by how much?
        layer_2_error = y - layer_2
        layer_2_deltas = rbind(layer_2_deltas, layer_2_error * sig_to_der(layer_2))
        overallError = overallError + abs(layer_2_error)

        # decode estimate so we can print it out
        d[position] = round(layer_2)

        # store hidden layer
        layer_1_values = rbind(layer_1_values, layer_1)
    }

    future_layer_1_delta = matrix(0, nrow = 1, ncol = hidden_dim)

    for (position in binary_dim:1) {
        X = cbind(a[position], b[position])
        layer_1 = layer_1_values[dim(layer_1_values)[1]-(binary_dim-position),]
        prev_layer_1 = layer_1_values[dim(layer_1_values)[1]- ( (binary_dim-position)+1 ),]

        # error at output layer
        layer_2_delta = layer_2_deltas[dim(layer_2_deltas)[1]-(binary_dim-position),]
        # error at hidden layer
        layer_1_delta = (future_layer_1_delta %*% t(weights_h) +
          layer_2_delta %*% t(weights_1)) * sig_to_der(layer_1)

        # let's update all our weights so we can try again
        weights_1_update = weights_1_update + matrix(layer_1) %*% layer_2_delta
        weights_h_update = weights_h_update + matrix(prev_layer_1) %*% layer_1_delta
        weights_0_update = weights_0_update + t(X) %*% layer_1_delta

        future_layer_1_delta = layer_1_delta
    }

    weights_0 = weights_0 + ( weights_0_update * alpha )
    weights_1 = weights_1 + ( weights_1_update * alpha )
    weights_h = weights_h + ( weights_h_update * alpha )

    weights_0_update = weights_0_update * 0
    weights_1_update = weights_1_update * 0
    weights_h_update = weights_h_update * 0

    if(j%%(training_data_size/10) == 0)
        print(paste("Error:", overallError))

}


```



## RNN-SURV package

http://medianetlab.ee.ucla.edu/papers/RNN_SURV.pdf

A deep recurrent model for survival analysis

 * Jointly minimize two loss functions
 * The modified cross-entropy function taking into account of the censored data
 * The second one is an upper bound of the negative C-index

https://jamanetwork.com/journals/jama/fullarticle/372568

 * C-index estimates the probability that, of two randomly choose patients, the patient with the higher prognostic score will outlive the patient with the lower prognostic score. Values of C near 0.5 indicate that the proganostic score is no better than a coin-flip.
 * Draw a pair of patients and determine which patient lived longer. If not comparable, e.g., don't who will live longer, then not used in the comparison.
 * Draw all possible pairs for which the ordering of survival times could be inferred.
 * C is the fraction of pairs that patients with higher prognostic score had longer survival time.
