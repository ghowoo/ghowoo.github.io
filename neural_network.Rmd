---
title: "Style"
author: "Wu Gong"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: false
    code_folding: hide
    toc_depth: 6
    lightbox: true
    gallery: false
    highlight: monochrome
    css: Wu.css
---



```{r setup, echo=FALSE, cache=FALSE,warning=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(Wu)
opts_chunk$set(echo=TRUE,
               cache=FALSE,
               eval=TRUE,
               prompt=FALSE,
               results="asis",
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               out.width = '80%')
eval_fast <- TRUE
eval_slow <- FALSE
```



# Index

[Index](index.html)


# Recurrent Neural Network


## A Basic Recurrent Neural Network

This is a reading note for
https://cran.r-project.org/web/packages/rnn/vignettes/basic_rnn.html

https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks



### Convert integer to bits

```{r}

intToBits(1)
intToBits(2)
intToBits(3)[1:4]

i2b <- function(integer, length=8)
  as.numeric(intToBits(integer))[1:length]

int2bin <- function(integer, length=8)
  t(sapply(integer, i2b, length=length))

```

### Generate Data

```{r}
training_data_size = 20000

# create sample inputs
X1 = sample(0:127, training_data_size, replace=TRUE)
X2 = sample(0:127, training_data_size, replace=TRUE)

# create sample output
Y <- X1 + X2

# convert to binary
X1 <- int2bin(X1)
X2 <- int2bin(X2)
Y  <- int2bin(Y)

class(X1)
dim(X1)


# create 3d array: dim 1: samples; dim 2: time; dim 3: variables
X <- array(c(X1,X2), dim=c(dim(X1),2))
class(X)
dim(X)

```

### Sigmoid function and its derivative function

 * Sigmoid function is a S-shape curve or sigmoid curve. Logistic function is an example of sigmoid function.
 * Derivative of sigmoid is a function of derivative of x.

```{r}

sigmoid <- function(x)
    1 / (1+exp(-x))

sig_to_der <- function(x)
    x*(1-x)

```

### Initial Weights

 * Input has two dimensions (two nodes or two predictors), X1 and X2
 * The hidder layer has six nodes
 * Output has only one node (corresponding to outcome y)

```{r}
binary_dim = 8
alpha      = 0.5
input_dim  = 2
hidden_dim = 6
output_dim = 1

# initialize weights randomly between -1 and 1, with mean 0
weights_0 = matrix(runif(n = input_dim *hidden_dim, min=-1, max=1),
                   nrow=input_dim,
                   ncol=hidden_dim )
weights_h = matrix(runif(n = hidden_dim*hidden_dim, min=-1, max=1),
                   nrow=hidden_dim,
                   ncol=hidden_dim )
weights_1 = matrix(runif(n = hidden_dim*output_dim, min=-1, max=1),
                   nrow=hidden_dim,
                   ncol=output_dim )

# create matrices to store updates, to be used in backpropagation
weights_0_update = matrix(0, nrow = input_dim,  ncol = hidden_dim)
weights_h_update = matrix(0, nrow = hidden_dim, ncol = hidden_dim)
weights_1_update = matrix(0, nrow = hidden_dim, ncol = output_dim)

```


### Matrix Operation

```{r}
dim(X)
dim(weights_0)

t1 <- cbind(X1[3, 6], X2[3, 6])
dim(t1)
t2 <- t1 %*% weights_0
dim(t2)

dim(weights_1)
t3 <- t2 %*% weights_1
dim(t3)
print(t3)
print(sigmoid(t3))


```



### Training Algorithmn

```{r}
# training logic
for (j in 1:training_data_size) {
    # select data
    a = X1[j,]
    b = X2[j,]

    # select true answer
    c = Y[j,]

    # where we'll store our best guesss (binary encoded)
    d = matrix(0, nrow = 1, ncol = binary_dim)

    overallError = 0

    layer_2_deltas = matrix(0)
    layer_1_values = matrix(0, nrow=1, ncol = hidden_dim)

    # moving along the positions in the binary encoding
    for (position in 1:binary_dim) {
        # generate input and output
        X = cbind( a[position], b[position] ) # rename X to layer_0?
        y = c[position]

        # hidden layer
        layer_1 = sigmoid( (X%*%weights_0) +
                    (layer_1_values[dim(layer_1_values)[1],] %*% weights_h) )

        # output layer
        layer_2 = sigmoid(layer_1 %*% weights_1)

        # did we miss?... if so, by how much?
        layer_2_error = y - layer_2
        layer_2_deltas = rbind(layer_2_deltas, layer_2_error * sig_to_der(layer_2))
        overallError = overallError + abs(layer_2_error)

        # decode estimate so we can print it out
        d[position] = round(layer_2)

        # store hidden layer
        layer_1_values = rbind(layer_1_values, layer_1)
    }

    future_layer_1_delta = matrix(0, nrow = 1, ncol = hidden_dim)

    for (position in binary_dim:1) {
        X = cbind(a[position], b[position])
        layer_1 = layer_1_values[dim(layer_1_values)[1]-(binary_dim-position),]
        prev_layer_1 = layer_1_values[dim(layer_1_values)[1]- ( (binary_dim-position)+1 ),]

        # error at output layer
        layer_2_delta = layer_2_deltas[dim(layer_2_deltas)[1]-(binary_dim-position),]
        # error at hidden layer
        layer_1_delta = (future_layer_1_delta %*% t(weights_h) +
          layer_2_delta %*% t(weights_1)) * sig_to_der(layer_1)

        # let's update all our weights so we can try again
        weights_1_update = weights_1_update + matrix(layer_1) %*% layer_2_delta
        weights_h_update = weights_h_update + matrix(prev_layer_1) %*% layer_1_delta
        weights_0_update = weights_0_update + t(X) %*% layer_1_delta

        future_layer_1_delta = layer_1_delta
    }

    weights_0 = weights_0 + ( weights_0_update * alpha )
    weights_1 = weights_1 + ( weights_1_update * alpha )
    weights_h = weights_h + ( weights_h_update * alpha )

    weights_0_update = weights_0_update * 0
    weights_1_update = weights_1_update * 0
    weights_h_update = weights_h_update * 0

    if(j%%(training_data_size/10) == 0)
        print(paste("Error:", overallError))

}


```
