---
title: "Random Forest"
author: "Wu Gong"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: false
    code_folding: hide
    toc_depth: 6
    lightbox: true
    gallery: false
    highlight: monochrome
    css: Wu.css
---



```{r setup, echo=FALSE, cache=FALSE,warning=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(Wu)
opts_chunk$set(echo=TRUE,
               cache=FALSE,
               eval=TRUE,
               prompt=FALSE,
               results="asis",
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               out.width = '80%')
eval_fast <- TRUE
eval_slow <- FALSE
library(reticulate)
py_home <- py_config()
use_virtualenv(py_home$virtualenv)
knitr::knit_engines$set(python = reticulate::eng_python)

```







# [Back to Index](index.html)


# Simulate Data


```{r}
library(data.table)

set.seed(123456)
n <- 5000
dt <- data.table(
    p0 = rep(0.2, n)
  , or1 = rep(1, n)
  , var1 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.3, 0.7))
  , var1n = rnorm(n, 0, 1)
  , or2 = rep(1.1, n)
  , var2 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.4, 0.6))
  , var2n = rnorm(n, 0, 2)
  , or3 = rep(1.2, n)
  , var3 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.2, 0.8))
  , var3n = rnorm(n, 0, 2)
  , or4 = rep(1.5, n)
  , var4 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.3, 0.7))
  , var4n = rnorm(n, 0, 2)
  , or5 = rep(1.7, n)
  , var5 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
  , var5n = rnorm(n, 0, 2)
  , or6 = rep(2, n)
  , var6 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.4, 0.6))
  , var6n = rnorm(n, 0, 2)
  , or7 = rep(5, n)
  , var7 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.1, 0.9))
  , var7n = rnorm(n, 0, 2)
)


dt <- dt[, odds0 := p0 / (1 - p0)
         ][, log_odds := log(odds0) +
                 var1 * log(or1) + var1n * log(or1) + 
                 var2 * log(or2) + var2n * log(or2) + 
                 var3 * log(or3) + var3n * log(or3) + 
                 var4 * log(or4) + var4n * log(or4) + 
                 var5 * log(or5) + var5n * log(or5) + 
                 var6 * log(or6) + var6n * log(or6) + 
                 var7 * log(or7) + var7n * log(or7)
           ][, p := exp(log_odds)/ (1 + exp(log_odds))]

vsample <- function(p){
    sample(c(1, 0), size = 1, replace = TRUE, prob = c(p, 1 - p))
}
vsample <- Vectorize(vsample)

dt <- dt[, outcome := vsample(p)][, outcome := factor(outcome, levels = c(0, 1))]

unique(dt[, .(or1, or2, or3, or4, or5, or6, or7)]) %>% prt(caption = "Variables with Odds Ratios")

library(feather)
write_feather(dt, "dt.feather")
```

# GLM

```{r}
m <- glm(outcome ~ var1 + var2 + var3 + var4 + var5 + var6 + var7 +
             var1n + var2n + var3n + var4n + var5n + var6n + var7n
       , data = dt
       , family = binomial
         )

library(sjPlot)

tab_model(m)

```



# Random Forest with R 


 * R [randomForest](https://cran.r-project.org/web/packages/randomForest/index.html) package


## Variable Importance

```{r, fig.height=21}


predictors <- c(
    "var1"
  , "var2"
  , "var3"
  , "var4"
  , "var5"
  , "var6"
  , "var7"
  , "var1n"
  , "var2n"
  , "var3n"
  , "var4n"
  , "var5n"
  , "var6n"
  , "var7n"
)

frml <- Wu::wu_formula(outcome = "", predictors = predictors)
mmx <- model.matrix.lm(frml, data = dt, na.action = "na.pass")
cn <- colnames(mmx)

frml <- Wu::wu_formula(outcome = "outcome", predictors = cn[-1])
dm <- cbind(dt[, .(outcome)], mmx[,-1])

library(randomForest)

set.seed(123456)
rf <- randomForest(frml
                 , data = dm
                 , ntree = 2000
                 , mtry = 2
                 , sampsize = ceiling(0.632 * nrow(dt))
                 , importance = TRUE
                 , na.action = na.roughfix
                   )


vi <- importance(rf, type = 1)
vit <- data.table(variable = attr(vi, "dimnames")[[1]], vi)
colnames(vit) <- c("variable", "MeanDecreaseAccuracy")

vit <- vit[order(MeanDecreaseAccuracy)]
vit <- vit[, variable := factor(variable, levels = vit$variable)]

plt_varimp(data = vit, var_name = variable
         , var_imp = MeanDecreaseAccuracy
         , xlabel = "Mean Decrease Accuracy"
         , ylabel = "Variable"
           )

```


## Partial Dependent Plot


```{r}
partialPlot(rf, pred.data = dm, x.var = "var1", which.class = "1")
partialPlot(rf, pred.data = dm, x.var = "var3n", which.class = "1")

```


## AUC on the Traning Data

```{r}
pred <- predict(object = rf, type = "prob")
library(pROC)
r <- roc(dm$outcome, pred[, 2], ci = TRUE, direction = "<")
plot(r)
plt_roc(r) %>% ann("AUC of Random Forest on the Training Data")

```


# Random Forest with Python

 * Settings
 ````
 library(reticulate)
 py_home <- py_config()
 use_virtualenv(py_home$virtualenv)
 knitr::knit_engines$set(python = reticulate::eng_python)
 ````
 * Install Packages
 ````
 library(reticulate)
 virtualenv_install("py39bert", "sklearn")
 ````
 * [sklearn - random forest](https://machinelearningmastery.com/random-forest-ensemble-in-python/)


## Load Data


```{python}
import os
path = os.getcwd()

os.chdir("/home/ghowoo/Works/stat")

import pandas as pd

df = pd.read_feather("/home/ghowoo/Works/stat/dt.feather")

from tabulate import tabulate
print(tabulate(df.head(), tablefmt = 'html'))
```

## Split

 * Split dataset into training set and test set
 * 70% training and 30% test


```{python}

X = df[["var1", "var2", "var3", "var4", "var5", "var6", "var7", "var1n", "var2n", "var3n", "var4n", "var5n", "var6n", "var7n"]]
y = df[["outcome"]]

from sklearn.model_selection import train_test_split
import numpy as np
np.random.seed(123456)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)


```

## Training

```{python}
#Import Random Forest Model
from sklearn.ensemble import RandomForestClassifier

#Create a Gaussian Classifier
clf=RandomForestClassifier(n_estimators=1000, criterion="entropy", bootstrap=True, max_features='sqrt', random_state=123456)

#print("Parameters currently in use:\n")
#print(clf.get_params())

#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(X_train,y_train)

y_pred=clf.predict(X_test)
#print(y_pred[0:10])

y_prob=clf.predict_proba(X_test)
#print(y_prob[0:10, 1])





from sklearn import metrics
print("Accuracy of the model: ", metrics.accuracy_score(y_test, y_pred))

#print(dir(clf))

#print(clf.feature_importances_)




import matplotlib.pyplot as plt
from sklearn.inspection import partial_dependence
from sklearn.inspection import PartialDependenceDisplay



```

## AUC

```{python}
from sklearn.metrics import roc_auc_score

roc_value = roc_auc_score(y_test, y_prob[:,1])
print(roc_value)

import matplotlib.pyplot as plt
from sklearn import metrics
metrics.plot_roc_curve(clf, X_test, y_test)
plt.show()
```


## Feature Importance

```{python}
import pandas as pd

feature_imp = pd.Series(clf.feature_importances_)
feature_names = pd.Series(clf.feature_names_in_)

di = {'variable': feature_names, 'importance': feature_imp}
di = pd.DataFrame(di)

print(tabulate(di, tablefmt='html'))

```

## Hyperparamters

```{python, eval=FALSE}

from sklearn.model_selection import RandomizedSearchCV
n_estimators =[int(x) for x in np.linspace(start=200,stop=2000,num=10)]
print(n_estimators)
max_features =['auto', 'sqrt']
max_depth=[int(x) for x in np.linspace(10, 110,num=11)]
print(max_depth)
max_depth.append(None)
min_samples_split=[2,5,10]
min_samples_leaf=[1,2,4]
bootstrap=[True,False]

random_grid={'n_estimators': n_estimators,
             'max_features': max_features,
             'max_depth': max_depth,
             'min_samples_split': min_samples_split,
             'min_samples_leaf': min_samples_leaf,
             'bootstrap': bootstrap}

import pprint as pp
pp.pprint(random_grid)

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier()
rf_ranfom=RandomizedSearchCV(estimator=rf
                             , param_distributions=random_grid
                             , n_iter=300
                             , cv =5
                             , verbose=2
                             , random_state = 123456
                             , n_jobs = -1)

rf_ranfom.fit(X_train, y_train)


rf_ranfom.best_params_

```

## Classification

```{python, eval=FALSE}

# evaluate random forest algorithm for classification
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
# define dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)
# define the model
model = RandomForestClassifier()

# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

```


## Prediction

```{python, eval=FALSE}
# make predictions using random forest for classification
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
# define dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)
# define the model
model = RandomForestClassifier()
# fit the model on the whole dataset
model.fit(X, y)
# make a single prediction
row = [[-8.52381793,5.24451077,-12.14967704,-2.92949242,0.99314133,0.67326595,-0.38657932,1.27955683,-0.60712621,3.20807316,0.60504151,-1.38706415,8.92444588,-7.43027595,-2.33653219,1.10358169,0.21547782,1.05057966,0.6975331,0.26076035]]
yhat = model.predict(row)
print('Predicted Class: %d' % yhat[0])

```

