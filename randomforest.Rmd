---
title: "Random Forest"
author: "Wu Gong"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: false
    code_folding: hide
    toc_depth: 6
    lightbox: true
    gallery: false
    highlight: monochrome
    css: Wu.css
---



```{r setup, echo=FALSE, cache=FALSE,warning=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(Wu)
opts_chunk$set(echo=TRUE,
               cache=FALSE,
               eval=TRUE,
               prompt=FALSE,
               results="asis",
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               out.width = '80%')
eval_fast <- TRUE
eval_slow <- FALSE
library(reticulate)
use_virtualenv("py39bert")
knitr::knit_engines$set(python = reticulate::eng_python)

```







# [Index](index.html)


# Install Packages

```{r}
library(reticulate)
## virtualenv_install("py39bert", "sklearn")



```

# sklearn



* Reference Link: https://machinelearningmastery.com/random-forest-ensemble-in-python/

## Create a classification data

```{python}
# test classification dataset
import sklearn
print(sklearn.__version__)

from sklearn.datasets import make_classification

# define dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)

# summarize the dataset
print(X.shape, y.shape)


print(X[0:2,:])

print(y[0:10])


```

## Split

```{python}

from sklearn.model_selection import train_test_split
import numpy as np
np.random.seed(20220214)
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
# 70% training and 30% test

```

## Training

```{python}
#Import Random Forest Model
from sklearn.ensemble import RandomForestClassifier

#Create a Gaussian Classifier
clf=RandomForestClassifier(n_estimators=1000, criterion="entropy", bootstrap=True, max_features='sqrt', random_state=20220214)

print("Parameters currently in use:\n")
print(clf.get_params())

#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(X_train,y_train)

y_pred=clf.predict(X_test)
print(y_pred[0:10])

y_prob=clf.predict_proba(X_test)
print(y_prob[0:10, 1])





from sklearn import metrics
print("Accuracy of the model: ", metrics.accuracy_score(y_test, y_pred))

print(dir(clf))

print(clf.feature_importances_)




import matplotlib.pyplot as plt
from sklearn.inspection import partial_dependence
from sklearn.inspection import PartialDependenceDisplay



```

## AUC

```{python}
from sklearn.metrics import roc_auc_score

roc_value = roc_auc_score(y_test, y_prob[:,1])
print(roc_value)

import matplotlib.pyplot as plt
from sklearn import metrics
metrics.plot_roc_curve(clf, X_test, y_test)
plt.show()
```


## Feature Importance

```{python}
import pandas as pd
#feature_imp = pd.series(clf.feature_importance_, index = )

```

## Hyperparamters

```{python}

from sklearn.model_selection import RandomizedSearchCV
n_estimators =[int(x) for x in np.linspace(start=200,stop=2000,num=10)]
print(n_estimators)
max_features =['auto', 'sqrt']
max_depth=[int(x) for x in np.linspace(10, 110,num=11)]
print(max_depth)
max_depth.append(None)
min_samples_split=[2,5,10]
min_samples_leaf=[1,2,4]
bootstrap=[True,False]

random_grid={'n_estimators': n_estimators,
             'max_features': max_features,
             'max_depth': max_depth,
             'min_samples_split': min_samples_split,
             'min_samples_leaf': min_samples_leaf,
             'bootstrap': bootstrap}

import pprint as pp
pp.pprint(random_grid)

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier()
rf_ranfom=RandomizedSearchCV(estimator=rf
                             , param_distributions=random_grid
                             , n_iter=300
                             , cv =5
                             , verbose=2
                             , random_state = 20220214
                             , n_jobs = -1)

rf_ranfom.fit(X_train, y_train)


rf_ranfom.best_params_

```

## Classification

```{python}

# evaluate random forest algorithm for classification
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
# define dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)
# define the model
model = RandomForestClassifier()

# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

```


## Prediction

```{python}
# make predictions using random forest for classification
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
# define dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)
# define the model
model = RandomForestClassifier()
# fit the model on the whole dataset
model.fit(X, y)
# make a single prediction
row = [[-8.52381793,5.24451077,-12.14967704,-2.92949242,0.99314133,0.67326595,-0.38657932,1.27955683,-0.60712621,3.20807316,0.60504151,-1.38706415,8.92444588,-7.43027595,-2.33653219,1.10358169,0.21547782,1.05057966,0.6975331,0.26076035]]
yhat = model.predict(row)
print('Predicted Class: %d' % yhat[0])

```

