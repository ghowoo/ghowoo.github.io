---
title: "SHAP: SHapley Additive exPlanations"
author: "Wu Gong"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: false
    code_folding: hide
    toc_depth: 6
    lightbox: true
    gallery: false
    highlight: monochrome
    css: Wu.css
---



```{r setup, echo=FALSE, cache=FALSE,warning=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(Wu)
opts_chunk$set(echo=TRUE,
               cache=FALSE,
               eval=TRUE,
               prompt=FALSE,
               results="asis",
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               out.width = '80%')
eval_fast <- TRUE
eval_slow <- FALSE
```



# [Index](index.html)



# Reference

 * https://github.com/slundberg/shap
 * https://cran.r-project.org/web/packages/shapper/shapper.pdf


# R shapper Package

https://cran.r-project.org/web/packages/shapper/vignettes/shapper_regression.html


## Load Data

```{r}
library("DALEX")
titanic_train <- titanic[,c("survived", "class", "gender", "age", "sibsp", "parch", "fare", "embarked")]
titanic_train$survived <- factor(titanic_train$survived)
titanic_train$gender <- factor(titanic_train$gender)
titanic_train$embarked <- factor(titanic_train$embarked)
titanic_train <- na.omit(titanic_train)
head(titanic_train) %>% prt()

Vars <- c(
  "class"
, "gender"
, "age"
, "sibsp"
, "parch"
, "fare"
, "embarked"
)


factorVars <- c(
  "class"
, "gender"
, "sibsp"
, "parch"
, "embarked"
)

tbl1n(data = titanic_train, vars = Vars, factorVars = factorVars) %>% prt()

```

## Build Model


```{r}
library("randomForest")
set.seed(123)
model_rf <- randomForest(survived ~ . , data = titanic_train)


model_rf

plot(model_rf)

model_rf$importance %>% prt()

varImpPlot(model_rf)

```


## shapper

```{r}


library("DALEX")
exp_rf <- explain(model_rf, data = titanic_train[,-1], y = as.numeric(titanic_train[,1])-1)


p_function <- function(model, data) predict(model, newdata = data, type = "prob")

library("shapper")

ive_rf <- individual_variable_effect(
  model_rf
, data = titanic_train[, -1]
, predict_function = p_function
, new_observation = titanic_train[1:2, -1]
, nsamples = 50
)

ive_rf %>% prt()

## plot(ive_rf)



```


# SHAPforxgboost Package

 * https://liuyanguu.github.io/post/2019/07/18/visualization-of-shap-for-xgboost/



## Parameter Searching

```{r, eval = FALSE}
library(SHAPforxgboost)
suppressPackageStartupMessages({
library("SHAPforxgboost"); library("ggplot2"); library("xgboost")
library("data.table"); library("here")
})

y_var <-  "diffcwv"
dataX <- as.matrix(dataXY_df[,-..y_var])
dataX <- xgb.DMatrix(dataX)

## cv1 <- xgb.cv(data = dataX )

library(rBayesianOptimization)
cv_folds <- KFold(y_var, nfolds = 5, stratified = FALSE, seed = 123456)
xgb_cv_bayes <- function(nround
                       , max.depth
                       , min_child_weight
                       , subsample
                       , eta
                       , gamma
                       , colsample_bytree
                       , max_delta_step) {
  param <- list(booster = "gbtree",
                max_depth = max.depth,
                min_child_weight = min_child_weight,
                eta=eta,gamma=gamma,
                subsample = subsample, colsample_bytree = colsample_bytree,
                max_delta_step=max_delta_step,
                lambda = 1, alpha = 0,
                ## objective = "binary:logistic",
                objective = "reg:squarederror",
                eval_metric = "auc")
  cv <- xgb.cv(params = param
             , data = dataX
             , label = y_var
             , folds = cv_folds
             , nrounds = 1000
             , early_stopping_rounds = 10
             , maximize = TRUE
             , verbose = verbose
               )
  list(Score = cv$evaluation_log$test_auc_mean[cv$best_iteration],
       Pred=cv$best_iteration)
}


OPT_Res <- BayesianOptimization(
  xgb_cv_bayes
, bounds = list(max.depth =c(3L, 10L)
               ,min_child_weight = c(1L, 40L),
                subsample = c(0.6, 0.9),
                eta=c(0.01,0.3),gamma = c(0.0, 0.2),
                colsample_bytree=c(0.5,0.8)
               ,max_delta_step=c(1L,10L))
, init_grid_dt = NULL
, init_points = 10
, n_iter = 10
, acq = "ucb"
, kappa = 2.576
, eps = 0.0
, verbose = verbose
)

best_param <- list(
booster = "gbtree",
eval.metric = "auc",
objective = "binary:logistic",
max_depth = OPT_Res$Best_Par["max.depth"],
eta = OPT_Res$Best_Par["eta"],
gamma = OPT_Res$Best_Par["gamma"],
subsample = OPT_Res$Best_Par["subsample"],
colsample_bytree = OPT_Res$Best_Par["colsample_bytree"],
min_child_weight = OPT_Res$Best_Par["min_child_weight"],
max_delta_step = OPT_Res$Best_Par["max_delta_step"])
# number of rounds should be tuned using CV
#https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/
# However, nrounds can not be directly derivied from the bayesianoptimization function
# Here, OPT_Res$Pred, which was supposed to be used for cross-validation, is used to record the number of rounds
nrounds=OPT_Res$Pred[[which.max(OPT_Res$History$Value)]]
xgb_model <- xgb.train (params = best_param, data = dtrain, nrounds = nrounds)

```


## Model


```{r, eval = FALSE}
library(SHAPforxgboost)
suppressPackageStartupMessages({
library("SHAPforxgboost"); library("ggplot2"); library("xgboost")
library("data.table"); library("here")
})

y_var <-  "diffcwv"
dataX <- as.matrix(dataXY_df[,-..y_var])

param_list <- list(objective = "reg:squarederror",  # For regression
                   eta = 0.02,
                   max_depth = 10,
                   gamma = 0.01,
                   subsample = 0.95
                   )


mod <- xgboost::xgboost(data = dataX, 
                        label = as.matrix(dataXY_df[[y_var]]), 
                        params = param_list, nrounds = 10,
                        verbose = FALSE, nthread = parallel::detectCores() - 2,
                        early_stopping_rounds = 8)

shap_values <- shap.values(xgb_model = mod, X_train = dataX)
# The ranked features by mean |SHAP|
shap_values$mean_shap_score %>% prt()


shap_values$shap_score[1:30,] %>% prt()


hist(shap_values$shap_score$forestProp_1km)


```


## SHAP Summary Plot


```{r, eval = FALSE}

shap_long <- shap.prep(xgb_model = mod, X_train = dataX)
# is the same as: using given shap_contrib
shap_long <- shap.prep(shap_contrib = shap_values$shap_score, X_train = dataX)

shap.plot.summary(shap_long)

shap.plot.summary.wrap1(model = mod, X = dataX)

shap.plot.summary.wrap2(shap_score = shap_values$shap_score, X = dataX)
```

## Dependence Plot

```{r, eval = FALSE}

g1 <- shap.plot.dependence(data_long = shap_long
                         , x = "dayint"
                         , y = "dayint"
                         , color_feature = "Column_WV") +
  ggtitle("(A) SHAP values of Time trend vs. Time Trend")

g2 <- shap.plot.dependence(data_long = shap_long
                         , x = "dayint"
                         , y = "Column_WV"
                         , color_feature = "Column_WV") +
  ggtitle("(B) SHAP values of CWV vs. Time Trend")

gridExtra::grid.arrange(g1, g2, ncol = 2)
```


## Interaction Effects

```{r, eval = FALSE}
fig_list <- lapply(names(shap_values$mean_shap_score)[1:9]
                 , shap.plot.dependence
                 , data_long = shap_long
                   )
gridExtra::grid.arrange(grobs = fig_list, ncol = 3)

```



# XGBoost with Python



## Data

```{r, eval = FALSE}
import pandas as pd
import feather


import os
print(os.getcwd())
os.chdir("/home/gongw/Works/VICTR/Patrick Doyle/R")
print(os.listdir())
print(os.getcwd())

path = "2yrs_matrix_train.feather"

df = pd.read_feather(path)



print(df.columns.values.tolist())

print(df.columns.values.tolist())

train_label = df['recurrence2yrs']
train_features = df.drop('recurrence2yrs',1)

print(train_features.columns.values.tolist())

print(train_features.shape)

patht = "2yrs_matrix_test.feather"

dft = pd.read_feather(patht)

print(dft.columns.values.tolist())

test_label = dft['recurrence2yrs']
test_features = dft.drop('recurrence2yrs',1)



```


## Model


```{r, eval = FALSE}

import xgboost as xgb
from datetime import datetime
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
import matplotlib.pyplot as plt
import pickle


xgb_cl_best = xgb.XGBClassifier(
    booster = "gbtree",
    objective = "binary:logistic",
    # eval_metric = "auc",
    n_estimators = 133,
    colsample_bytree = 0.85,
    subsample = 0.53,
    learning_rate = 0.16,
    max_depth = 4,
    gamma = 2.9,
    reg_lambda = 1,
    reg_alpha = 0.58,
    n_jobs = -1
)


eval_set = [(test_features, test_label)]

xgb_cl_best.fit(train_features,
                train_label,
                eval_metric = "auc",
                eval_set=eval_set,
                early_stopping_rounds = 50,
                verbose = True
                )


with open('xgb_2yrs.pkl', 'wb') as outp:
    pickle.dump(xgb_cl_best, outp, pickle.HIGHEST_PROTOCOL)

    

xgb_cl_best


```


## AUC & Variable Importance


```{r, eval = FALSE}

from sklearn import metrics
pred_train = xgb_cl_best.predict_proba(train_features)[:,1]
pred_train_pd = pd.DataFrame({'prob':pred_train})
pred_train_pd.to_feather("pred_train_2yrs_xgb.feather")


fpr, tpr, threshold = metrics.roc_curve(train_label, pred_train)
roc_auc = metrics.auc(fpr, tpr)
roc_auc


pred_test = xgb_cl_best.predict_proba(test_features)[:,1]
pred_test_pd = pd.DataFrame({'prob':pred_test})
pred_test_pd.to_feather("pred_test_2yrs_xgb.feather")


fpr, tpr, threshold = metrics.roc_curve(test_label, pred_test)
roc_auc = metrics.auc(fpr, tpr)
roc_auc



importances_weight = xgb_cl_best.get_booster().get_score(importance_type = 'weight')

# importances = xgb_cl_best.feature_importances_
print(importances_weight)

importance_weight_xgb_2yrs = pd.DataFrame([importances_weight]).transpose()
importance_weight_xgb_2yrs.index.name = "variable"
importance_weight_xgb_2yrs.reset_index(inplace=True)
importance_weight_xgb_2yrs.rename(columns={0: "importance_weight"}, inplace=True)


print(importance_weight_xgb_2yrs)
print(importance_weight_xgb_2yrs.shape)



# importance_weight_xgb_2yrs = pd.DataFrame({'value':importances_weight, "variable":train_features.columns.values})

importance_weight_xgb_2yrs.to_feather("importance_weight_xgb_2yrs.feather")


print(xgb_cl_best.feature_importances_)


# permutation importance
from sklearn.inspection import permutation_importance
importance_permutation = permutation_importance(xgb_cl_best, train_features, train_label)
print(importance_permutation.importances_mean)

importance_permutation_xgb_2yrs = pd.DataFrame({'value':importance_permutation.importances_mean, "variable":train_features.columns.values})


importance_permutation_xgb_2yrs.to_feather("importance_permutation_xgb_2yrs.feather")


```


## SHAP

```{r, eval = FALSE}
import shap

explainer = shap.TreeExplainer(xgb_cl_best)
shap_values = explainer.shap_values(train_features)
print(shap_values)

shap.summary_plot(shap_values, train_features, plot_type="bar")

shap.summary_plot(shap_values, train_features)


```
