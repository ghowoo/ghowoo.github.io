---
title: "Time-To-Event Analysis"
author: "Wu Gong"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: true
    code_folding: hide
    toc_depth: 6
    lightbox: true
    gallery: false
    highlight: tango
---


```{r setup, echo=FALSE, cache=FALSE,warning=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(Wu)
## Global options
options(max.print="2000")
opts_chunk$set(echo=TRUE,
               eval=TRUE,
               cache=FALSE,
               results="asis",
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
eval_fast <- TRUE
eval_slow <- FALSE

```

# Weibull AFT Regression in R

[Sarah R Haile: Weibull AFT Regression Functions in R](https://cran.r-project.org/web/packages/SurvRegCensCov/vignettes/weibull.pdf "Sarah Haile: Weibull AFT Regression in R")


Accelerated Failure Time model:

$$ logT = Y = \mu + \alpha^{T}z + \sigma W $$

W has the extreme value distribution.

[Extreme Value Distribution](https://www.itl.nist.gov/div898/handbook/apr/section1/apr163.htm)

* Extreme value distributions arise as limiting distributions for maximums or minimums (extreme values) of a sample of independent, identically distributed random variables, as the sample size increases.
* In the context of reliability modeling, extreme value distributions for the minimum are frequently encountered. For example, if a system consists of n identical components in series, and the system fails when the first of these components fails, then system failure times are the minimum of n random component failure times.
* Extreme value theory says that, independent of the choice of component model, the system model will approach a Weibull as n becomes large.
* The same reasoning can also be applied at a component level, if the component failure occurs when the first of many similar competing failure processes reaches a critical level.


$$ \gamma = 1 / \sigma $$
$$ \lambda =  exp(-\mu/\sigma)$$
$$ \beta = -\alpha/\sigma $$

The Hazard function of the Weibull model, the first part is the baseline hazard.

$$ h(x|z) = (\gamma \lambda t^{\gamma - 1} ) exp(\beta^{T} z) $$


[Weibull](https://stat.ethz.ch/education/semesters/ss2011/seminar/contents/handout_9.pdf)

* $ \gamma $ is a shape parameter
* $ \gamma > 1 $ the hazard increases
* $ \gamma = 1 $ the hazard is constant, Weibull reduces to exponential distribution
* $ \gamma < 1 $ the hazard decreases




Hazard Ratio:
$$ exp(\beta_i) $$

Event Time Ratio (ETR): the ratio quantifies the relative difference in time it takes to achieve the pth percentile between two lelvels of a covariate
$$ exp(\alpha_i) = exp(-\beta_i/\gamma) $$

```{r}
library(survival)
library(KMsurv)
library(SurvRegCensCov)
data(larynx)

m <- WeibullReg(Surv(time, death) ~ factor(stage) + age, data=larynx)

formula(m)
m$coef


```

# Weibull Distribution


```{r}
scale <- 1
shape <- 1
x <- seq(0, 5, by=0.1)
yd <- dweibull(x, shape=shape, scale=scale)

plot(x, yd)


```



# Simulating Survival Data

<https://stats.stackexchange.com/questions/135124/how-to-create-a-toy-survival-time-to-event-data-with-right-censoring>


```{r}
# baseline hazard: Weibull

# N = sample size
# lambda = scale parameter in h0()
# rho = shape parameter in h0()
# beta = fixed effect parameter
# rateC = rate parameter of the exponential distribution of C


simulWeib <- function(N, lambda, rho, beta, rateC)
{
  # covariate --> N Bernoulli trials
  x <- sample(x=c(0, 1), size=N, replace=TRUE, prob=c(0.5, 0.5))

  # Weibull latent event times
  v <- runif(n=N)
  Tlat <- (- log(v) / (lambda * exp(x * beta)))^(1 / rho)

  # censoring times
  C <- rexp(n=N, rate=rateC)

  # follow-up times and event indicators
  time <- pmin(Tlat, C)
  status <- as.numeric(Tlat <= C)

  # data set
  data.frame(id=1:N,
             time=time,
             status=status,
             x=x)
}

```

# Weibull with R survreg Package

* Set scale=0, the scale is estimated


```{r}
survreg(Surv(futime, fustat) ~ ecog.ps + rx
      , ovarian
      , dist='weibull'
      , scale=1
        )

survreg(Surv(futime, fustat) ~ ecog.ps + rx
      , ovarian
      , dist='weibull'
      , scale= 0
        )

```

# AFT Model

[AFT Model](https://stat.ethz.ch/education/semesters/ss2011/seminar/contents/handout_9.pdf)

$$ log(T) = \alpha_0 + \alpha_1 X + \epsilon$$

* log(T) = extreme value distribution: Exponential
* log(T) = extreme value distribution: Weibull, which has an additional parameter that scales $\epsilon$
* log(T) = logistic: log-logistic
* long(T) = normal: log-normal

* Accelerated time failure assumption: The probability of a dog surviving past t years is equal to a human surviving past 7*t years.
* AFT Models describe stretching out or contraction of survival time as a function of predictor variables.
* AFT assumption: S is survival, $\gamma$ is acceleration factor
$$ S_{non-treatment} = S_{treatment}(\gamma t)$$
$$ \gamma T_{non-treatment} = T_{treatment}$$
* $\gamma > 1$ exposure benefits survival




# Discrete Time Proportional Odds

## Data Simulation

* Assume average daily death rate of 5%
* Individual baseline death rate has a random effect has standard deviation of the half of the log odds
* The treatment effect of odds ratio 0.5
* Subjects are observed for 28 days


```{r}
library(Wu)

set.seed(123456)
n <- 1000
n0 <- 500
n1 <- n - n0

r_0 <- 0.05
odds_0 <- r_0 / (1 - r_0)
sd <- abs(log(odds_0) / 2)
odds_0s <- exp(rnorm(n0, log(odds_0), sd))
r_0s <- odds_0s / (1 + odds_0s)

or <- 0.5
odds_1 <- odds_0 * or
r_1 <- odds_1 / (1 + odds_1)
odds_1s <- exp(rnorm(n1, log(odds_1), sd))
r_1s <- odds_1s / (1 + odds_1s)

days <- 28
for(i in 1:days){
    print(i)
    event_i <- c(rbinom(n0, 1, r_0s), rbinom(n1, 1, r_1s))
    if(i == 1){
        t <- data.table(
            day=i
          , id=1:n
          , trt=rep(c(0, 1), each=c(n0, n1))
          , event=event_i
        )
        rtn <- copy(t)
    }else{
        event_i[t$event %in% c(1, NA)] <- NA
        t <- data.table(
            day=i
          , id=1:n
          , trt=rep(c(0, 1), each=c(n0, n1))
          , event=event_i
        )
        rtn <- rbind(rtn, t)
    }
}

dim(rtn) %>% prt()
rtn[, .N, by = .(day)][order(day)] %>% prt()
rtn[, .N, by = .(day, trt)][order(day)] %>% prt()
rtn[, .N, by = .(day, event)][order(day, event)] %>% prt()

dt <- copy(rtn)[!is.na(event)]
dim(dt)

dt <- dt[order(id, -day)
         ][, rn := 1:.N, by = .(id)
           ][rn == 1
             ][, rn := NULL]
dt[, .N, by = .(day, event)][order(day, event)]
dt <- dt[, censor := case_when(event == 0 ~ 1, TRUE ~ 0)]

```

## Kaplan Meier Plot

```{r}
library(survival)
library(survminer)

f <- survfit(Surv(day, event) ~ trt, data = dt)
plot(f)

ggsurvplot(f)

```

## Longitudinal Mixed-Effect Model R::lme4

```{r}
library(lme4)
dt <- copy(rtn)
dt <- dt[!is.na(event)]

t <- get_ors(outcome="event", predictor="trt", data=dt, digits = 3)
t

m <- glmer(event ~ trt + (1 | id)
         , data=dt
         , family = binomial(link=logit)
           )


summary(m)

exp(fixef(m))



```

## Bayesian Longitudinal Mixed-Effect Model R::brms

```{r}
library(brms)
dt <- copy(rtn)
dt <- dt[!is.na(event)]

m <- brm(event ~ trt + (1 | id)
       , data=dt
       , family = binomial(link=logit)
       , chain=5
       , cores=5
       , warmup=200
       , iter = 400
         )

summary(m)
exp(fixef(m))
ranef(m)

```


# Computing Environment


```{r}
sessionInfo()

```
