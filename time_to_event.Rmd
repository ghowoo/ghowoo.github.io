---
title: "Time-To-Event Analysis"
author: "Wu Gong"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: false
    code_folding: hide
    toc_depth: 6
    lightbox: true
    gallery: false
    highlight: monochrome
    css: Wu.css
---


```{r setup, echo=FALSE, cache=FALSE,warning=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(Wu)
opts_chunk$set(echo=TRUE,
               cache=FALSE,
               eval=TRUE,
               prompt=FALSE,
               results="asis",
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               out.width = '80%')
eval_fast <- TRUE
eval_slow <- FALSE

```


# Log-rank Test



## A simplest example


 * Ref: https://towardsdatascience.com/survival-analysis-in-clinical-trials-log-rank-test-8f1229e7f0f0
 * Ref: https://www.real-statistics.com/survival-analysis/kaplan-meier-procedure/kaplan-meier-comparison-tests/
 * Ref: https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Kaplan-Meier_Curves-Logrank_Tests.pdf
 * The test is based on $K - 1$ statistics, where $k$ represents one of the $K$ population (groups be compared)
 $$Z_k (\tau) = \sum_{T} W(T_i) r_{ik} (\frac{d_{ik}}{r_{ik}} - \frac{d_i}{r_i})$$
 
 where $r_i$ is the number of subjects at risk at time $T_i$, $r_{ik}$ is the subjects from group $k$
 and $d_i$ is the number of events at the time,
 When $W(T_I) = 1$, it is the log-rank test.
 
 * The $Z$s have a covariance matrix
 
 $$\sigma = \sum_T W(T_I)^2 (\frac{r_{ik}}{r_i})(\delta_{kg} - \frac{r_{ig}}{r_i})(\frac{r_i - d_i}{r_i - 1})d_i$$
 
 
 * Mantel-Haenszel Logrank Test
 Formula of log-rank statistic with two exposure groups $i = A, B$
 $$\chi^2_{MHL} = (\frac{O_A - E_A}{\sqrt{Var}})^2 \approx \chi_1^2$$
  Variance of hypergeometric distribution, if there are two groups:
 $$Var = \frac{n_{rowsum1} \cdot n_{rowsum2} \cdot n_{colsum1} \cdot n_{colsum2} }{N^2 \cdot (N - 1)}$$

 * Cox-Mantel Logrank Test for two groups
 $$\chi^2_{CM} = \frac{(O_A - E_A)^2}{E_A} + \frac{(O_B - E_B)^2}{E_B}$$


```{r}

library(survival)
df <- data.frame(
    treatment = c(0, 1)
  , time = c(1, 2)
  , event = c(1, 1)
)

survdiff(Surv(time, event) ~ treatment, data = df, rho = 0)


library(nph)

logrank.test(time = df$time, event = df$event, group = df$treatment)


library(survival)
df <- data.frame(
    treatment = c(0, 1)
  , time = c(1, 2)
  , event = c(1, 0)
)

survdiff(Surv(time, event) ~ treatment, data = df, rho = 0)



pchisq(q = 1, df = 1, lower.tail = TRUE)



```

## A normal example

 * Ref: https://web.stanford.edu/~lutian/coursepdf/survweek3.pdf

```{r}
dt <- data.table(
    group = c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1)
  , time = c(3.1, 6.8, 9, 9, 11.3, 16.2, 8.7, 9, 10.1, 12.1, 23.1)
  , event = c(0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1)
)

dt <- dt[order(group, time, event)
         ][, seq_i := 1:.N, by = .(group)
           ][, n_ij := .N, by = .(group, time)
             ][, n_group := .N, by = .(group)]



dtt <- dt[, .(n_group = max(n_group)
            , n_ij = max(n_ij)
              , n_at_risk_ij = max(n_group) - max(seq_i) + max(n_ij)
            , o_ij = sum(event, na.rm = TRUE)
              ), by = .(time, group)]

print(dtt)


dtt <- dtt[, n_j := sum(n_ij, na.rm = TRUE), by = .(time)
           ][, o_j := sum(o_ij, na.rm = TRUE), by = .(time)
             ][, e_j := o_j]




```

# Kaplan-Meier

## Simulating Data

```{r}
library(coxed)
set.seed(123456)

N = 1000
X <- data.frame(
    fac1 = sample(x = c(0, 1), size = N, replace = TRUE, prob = c(0.6, 0.4))
  , fac2 = rep(c(0, 1), each = N / 2)
  , group = rep(1:(N / 2), 2)
)

surv_obj <- sim.survdata(N = N
                 , T = 500
                 , X = X
                 , hazard.fun = NULL
                 , beta = log(c(2, 2, 1))
                 , censor = .2
                 , num.data.frames = 1
                   )

surv_obj$data[1:10, ] %>% prt()

```

## Product-limit estimate of survival function

 * Ref: https://www.math.wustl.edu/~sawyer/handouts/greenwood.pdf
 * Survival function: $$S(t) = Pr(T>t)$$
 * Kaplan-Meier estimator: $$\hat{S}(t) = \prod \limits_{t_i \le t} (1 - \frac{d_i}{n_i})$$
 * Greenwood's confidence interval: $$\hat{Var}[\hat{S}(t)] = \hat{S}(t)^2 \sum\limits_{t_i \le t} \frac{d_i}{n_i(n_i - d_i)}$$ 
 * Exponential Greenwood confidence interval (guaranteed to lie in 0-1):
 $$log(-log\hat{S}(t)) \pm z_{\alpha/2}\sqrt{\hat{V}}$$
 $$\hat{V} = \frac{1}{(log\hat{S}(t))^2} \sum \limits_{t_i \le t} \frac{d_i}{n_i(n_i - d_i)}$$

## Kaplan Meier Plot by survminer


```{r}
library(survival)
library(survminer)
library(sjPlot)

dt <- surv_obj$data
setDT(dt)
dt$group <- X$group
dt <- dt[, fac1 := factor(fac1, levels = c(0, 1))]


S <- survival::Surv(time = dt$y, event = dt$failed)
km <- survfit(S ~ fac1, type="kaplan-meier", conf.type="log", data = dt)


plot(survfit(Surv(dt$y, dt$failed) ~ dt$fac1))

ggsurvplot(km
         , xlab = "time"
         , xscale = 1
         , break.x.by = 100
         , title = "Kaplan-Meier Plot"
         , conf.int = TRUE
         , surv.median.line = "hv"
         , risk.table = TRUE
           )


```


## Logrank Test



```{r}
survdiff(Surv(y, failed) ~ fac2, data = dt)


m_cox <- coxph(S ~ fac1, data = dt, ties = "exact")
ms_cox <- summary(m_cox)
ms_cox$sctest



```

## Stratified Log-rank Test

```{r}
survdiff(Surv(y, failed) ~ fac2 + strata(group), data = dt)

library(nph)

logrank.test(dt$y, dt$failed, dt$group)

dt[group %in% c(1, 2, 3)][order(group)]

```



## Hall-Wellner simultaneous confidence bands

 * Ref: https://www.karlin.mff.cuni.cz/~vavraj/cda/exercise_04.html

```{r}
library("km.ci")
fit <- survfit(Surv(time,status)~1, data=aml, conf.type="plain", conf.int=0.95)
out <- km.ci(fit, conf.level=0.95, tl=5, tu=48, method="hall-wellner") 
par(mar = c(4,4,1,1))
plot(out, xlab = "Time", ylab = "Survival Probability")

```

# Confidence Band

 * Ref:  https://www.nature.com/articles/s41416-022-01920-5
 * https://scholar.princeton.edu/sites/default/files/mikkelpm/files/conf_band.pdf
 * For a one dimensional variable there is a confidence interval, and for a two dimensional plot there is a confidence band.
 * Confidence bands is a collections of confidence intervals for each component of a parameter vector.
 * Pointwise confidence band: the confidence interval for the survival probability at each time point
 $$ \hat{S}(t_i) \pm z_{1 - \alpha/2} \hat{se} (t_i)$$
 * Simutaneous confidence band: the probability of $f(x_i)$ is within the intervals for all of $x_i$ . Assuming $n$ times of simulation, less than $\alpha$ percent of $n$ has any of $f(x_i)$ locates outside of the intervals (band).
 
 

# Proportional Hazard

## Simulating Data


```{r}
library(coxed)
library(data.table)
library(survminer)
set.seed(123456)

N = 1000
X <- data.frame(
    fac1 = sample(x = c(0, 1), size = N, replace = TRUE, prob = c(0.6, 0.4))
  , fac2 = rep(c(0, 1), each = N / 2)
  , group = rep(1:(N / 2), 2)
)

surv_obj <- sim.survdata(N = N
                 , T = 500
                 , X = X
                 , hazard.fun = NULL
                 , beta = log(c(2, 2, 1))
                 , censor = .2
                 , num.data.frames = 1
                   )
dt <- surv_obj$data
setDT(dt)
dt$group <- X$group
dt <- dt[, fac1 := factor(fac1, levels = c(0, 1))]


```

## Cox Model

```{r}
S <- survival::Surv(time = dt$y, event = dt$failed)
m_cox <- coxph(S ~ fac1, data = dt)

tab_model(m_cox)

```

## Testing proportional Hazards Assumption


 * cox.zph() tests corresponding scaled Schoenfeld residuals with time
 * The Schoenfeld residuals are the differences between one individual's covariates values at the event time and the corresponding risk-weighted average of covariate values among all then at risk.
 * Schoenfeld's residuals are defined as
 $$\hat{r}_{(i)} = Z_{(i)} - \frac{\sum Z_j exp(\hat{\beta} Z_j)}{\sum exp(\hat{\beta}Z_j)}$$

 where $Z_{i}$ is the covaraite vector at time $i$, and the weight is the risk at the time (hazard) considering all covaraites (each individual has different hazard at the time)

```{r}
test.ph <- cox.zph(m_cox)
test.ph

ggcoxzph(test.ph)

```


## Cox Model with mgcv


```{r}
library(mgcv)

m_gam <- gam(y ~ fac1
           , data = dt
           , family = cox.ph()
           , weights = failed
             )
beta <- m_gam$coefficients
se <- summary(m_gam)$se

exp(c(beta, beta - qnorm(.975) * se, beta + qnorm(0.975) * se))


```


# Sample Size

## Sample size for the proportional hazards model


 * Schoenfeld: Sample-Size Formula for the Proportional-Hazards Regression Model
 * The total number of events required:
 $$(Z_{\beta} + z_{1 - \alpha})^2/(P_A P_B log_e^2\delta)$$
 * $P_A$ is the proportion of the patients randomized to the treatment A group
 * $\delta$ is the hazard ratio (A as reference)


```{r}
alpha = 0.05
beta = 0.8
q1 = 0.66
q0 = 0.34
hr = 0.4

z_beta <- qnorm(beta)
z_1_alpha <- qnorm(1 - alpha / 2)



ceiling((z_beta + z_1_alpha) ^ 2 / (q1 * q0 * (log(hr)) ^ 2))

library(powerSurvEpi)

expected_proportion_of_events <- 0.03
ssizeEpi.default(power = 0.8
               , theta = 0.4
               , p = 0.66
               , psi = expected_proportion_of_events
               , rho2 = 0
               , alpha = 0.05) * expected_proportion_of_events


library(npsurvSS)

arm0 <- create_arm(size = 200, accr_time = 1, surv_scale = 0.06, loss_scale = 0.001, follow_time = 12)
arm1 <- create_arm(size = 400, accr_time = 1, surv_scale = 0.03, loss_scale = 0.001, follow_time = 12)

power_two_arm(arm0, arm1, alpha = 0.05, sides = 2, test = list(test = "weighted logrank"))


```

# Example from mgcv

 * https://rdrr.io/cran/mgcv/man/coxph.html

```{r}
library(mgcv)
library(survival) ## for data
col1 <- colon[colon$etype==1,] ## concentrate on single event
col1$differ <- as.factor(col1$differ)
col1$sex <- as.factor(col1$sex)

b <- gam(time~sex,
         family=cox.ph(),data=col1,weights=status)

summary(b) 


```

# Mixed-effect Cox Model


## Simulate data with coxed

 * Ref: https://stats.oarc.ucla.edu/r/dae/mixed-effects-cox-regression/
 

```{r}
library(coxed)
set.seed(123456)

N = 1000
X <- data.frame(
    con1 = rnorm(n = N, sd = 1)
  , con2 = rnorm(n = N, sd = 1)
  , con3 = rnorm(n = N, sd = 2)
  , con4 = rnorm(n = N, sd = 2)
  , fac1 = sample(x = c(0, 1), size = N, replace = TRUE, prob = c(0.3, 0.7))
  , fac2 = sample(x = c(0, 1), size = N, replace = TRUE, prob = c(0.3, 0.7))
  , fac3 = sample(x = c(0, 1), size = N, replace = TRUE, prob = c(0.7, 0.3))
  , fac4 = sample(x = c(0, 1), size = N, replace = TRUE, prob = c(0.7, 0.3))
)

surv_obj <- sim.survdata(N = N
                 , T = 500
                 , X = X
                 , beta = log(c(1, 0.5, 1, 0.5, 1, 2, 1, 2))
                 , censor = .2
                 , num.data.frames = 1
                   )

head(surv_obj$data) %>% prt()

```


## R survival package

```{r}
df <- surv_obj$data
library(survival)
m <- coxph(Surv(y, failed) ~ fac2, data = df)
summary(m)


```

# Weibull AFT Regression in R

[Sarah R Haile: Weibull AFT Regression Functions in R](https://cran.r-project.org/web/packages/SurvRegCensCov/vignettes/weibull.pdf "Sarah Haile: Weibull AFT Regression in R")


Accelerated Failure Time model:

$$ logT = Y = \mu + \alpha^{T}z + \sigma W $$

W has the extreme value distribution.

[Extreme Value Distribution](https://www.itl.nist.gov/div898/handbook/apr/section1/apr163.htm)

* Extreme value distributions arise as limiting distributions for maximums or minimums (extreme values) of a sample of independent, identically distributed random variables, as the sample size increases.
* In the context of reliability modeling, extreme value distributions for the minimum are frequently encountered. For example, if a system consists of n identical components in series, and the system fails when the first of these components fails, then system failure times are the minimum of n random component failure times.
* Extreme value theory says that, independent of the choice of component model, the system model will approach a Weibull as n becomes large.
* The same reasoning can also be applied at a component level, if the component failure occurs when the first of many similar competing failure processes reaches a critical level.



$$ \gamma = 1 / \sigma $$

$$ \lambda =  exp(-\mu/\sigma)$$

$$ \beta = -\alpha/\sigma $$

The Hazard function of the Weibull model, its first part is the baseline hazard.

$$ h(x|z) = (\gamma \lambda t^{\gamma - 1} ) exp(\beta^{T} z) $$


[Weibull](https://stat.ethz.ch/education/semesters/ss2011/seminar/contents/handout_9.pdf)

* $ \gamma $ is a shape parameter
* $ \gamma > 1 $ the hazard increases
* $ \gamma = 1 $ the hazard is constant, Weibull reduces to exponential distribution
* $ \gamma < 1 $ the hazard decreases




Hazard Ratio:
$$ exp(\beta_i) $$

Event Time Ratio (ETR): the ratio quantifies the relative difference in time it takes to achieve the pth percentile between two lelvels of a covariate
$$ exp(\alpha_i) = exp(-\beta_i/\gamma) $$

```{r}
library(survival)
library(KMsurv)
library(SurvRegCensCov)
data(larynx)

m <- WeibullReg(Surv(time, death) ~ factor(stage) + age, data=larynx)

formula(m)
m$coef


```

# Weibull Distribution


```{r}
scale <- 1
shape <- 1
x <- seq(0, 5, by=0.1)
yd <- dweibull(x, shape=shape, scale=scale)

plot(x, yd)


```



# Simulating Survival Data

<https://stats.stackexchange.com/questions/135124/how-to-create-a-toy-survival-time-to-event-data-with-right-censoring>


```{r}
# baseline hazard: Weibull

# N = sample size
# lambda = scale parameter in h0()
# rho = shape parameter in h0()
# beta = fixed effect parameter
# rateC = rate parameter of the exponential distribution of C


simulWeib <- function(N, lambda, rho, beta, rateC)
{
  # covariate --> N Bernoulli trials
  x <- sample(x=c(0, 1), size=N, replace=TRUE, prob=c(0.5, 0.5))

  # Weibull latent event times
  v <- runif(n=N)
  Tlat <- (- log(v) / (lambda * exp(x * beta)))^(1 / rho)

  # censoring times
  C <- rexp(n=N, rate=rateC)

  # follow-up times and event indicators
  time <- pmin(Tlat, C)
  status <- as.numeric(Tlat <= C)

  # data set
  data.frame(id=1:N,
             time=time,
             status=status,
             x=x)
}

```

# Weibull with R survreg Package

* Set scale=0, the scale is estimated


```{r}
library(survival)

library(survival)
library(KMsurv)
library(SurvRegCensCov)
library(sjPlot)
data(larynx)

wr <- WeibullReg(Surv(time, death) ~ factor(stage) + age
            , data=larynx)
summary(wr)
wr$HR
wr$coef
wr$ETR

wb <- survreg(Surv(time, death) ~ factor(stage) + age
            , data=larynx, dist = "weibull")

summary(wb)
tab_model(wb)

scale <- wb$scale
shape <- 1 / scale
exp(coef(wb)) ^ shape

str(wb)



wb$scale * coef(wb)

coef(wb)




survreg(Surv(futime, fustat) ~ ecog.ps + rx
      , ovarian
      , dist='weibull'
      , scale=1
        )

survreg(Surv(futime, fustat) ~ ecog.ps + rx
      , ovarian
      , dist='weibull'
      , scale= 0
        )

```

# AFT Model

[AFT Model](https://stat.ethz.ch/education/semesters/ss2011/seminar/contents/handout_9.pdf)

$$ log(T) = \alpha_0 + \alpha_1 X + \epsilon$$

* log(T) = extreme value distribution: Exponential
* log(T) = extreme value distribution: Weibull, which has an additional parameter that scales $\epsilon$
* log(T) = logistic: log-logistic
* long(T) = normal: log-normal

* Accelerated time failure assumption: The probability of a dog surviving past t years is equal to a human surviving past 7*t years.
* AFT Models describe stretching out or contraction of survival time as a function of predictor variables.
* AFT assumption: S is survival, $\gamma$ is acceleration factor
$$ S_{non-treatment} = S_{treatment}(\gamma t)$$
$$ \gamma T_{non-treatment} = T_{treatment}$$
* $\gamma > 1$ exposure benefits survival




# Discrete Time Proportional Odds

## Data Simulation

* Assume average daily death rate of 5%
* Individual baseline death rate has a random effect has standard deviation of the half of the log odds
* The treatment effect of odds ratio 0.5
* Subjects are observed for 28 days


```{r}
library(Wu)

set.seed(123456)
n <- 1000
n0 <- 500
n1 <- n - n0

r_0 <- 0.05
odds_0 <- r_0 / (1 - r_0)
sd <- abs(log(odds_0) / 2)
odds_0s <- exp(rnorm(n0, log(odds_0), sd))
r_0s <- odds_0s / (1 + odds_0s)

or <- 0.5
odds_1 <- odds_0 * or
r_1 <- odds_1 / (1 + odds_1)
odds_1s <- exp(rnorm(n1, log(odds_1), sd))
r_1s <- odds_1s / (1 + odds_1s)

days <- 28
for(i in 1:days){
    print(i)
    event_i <- c(rbinom(n0, 1, r_0s), rbinom(n1, 1, r_1s))
    if(i == 1){
        t <- data.table(
            day=i
          , id=1:n
          , trt=rep(c(0, 1), each=c(n0, n1))
          , event=event_i
        )
        rtn <- copy(t)
    }else{
        event_i[t$event %in% c(1, NA)] <- NA
        t <- data.table(
            day=i
          , id=1:n
          , trt=rep(c(0, 1), each=c(n0, n1))
          , event=event_i
        )
        rtn <- rbind(rtn, t)
    }
}

dim(rtn) %>% prt()
rtn[, .N, by = .(day)][order(day)] %>% prt()
rtn[, .N, by = .(day, trt)][order(day)] %>% prt()
rtn[, .N, by = .(day, event)][order(day, event)] %>% prt()

dt <- copy(rtn)[!is.na(event)]
dim(dt)

dt <- dt[order(id, -day)
         ][, rn := 1:.N, by = .(id)
           ][rn == 1
             ][, rn := NULL]
dt[, .N, by = .(day, event)][order(day, event)]
dt <- dt[, censor := case_when(event == 0 ~ 1, TRUE ~ 0)]

```

## Kaplan Meier Plot

```{r}
library(survival)
library(survminer)

f <- survfit(Surv(day, event) ~ trt, data = dt)
plot(f)

ggsurvplot(f)

```

## Longitudinal Mixed-Effect Model R::lme4

```{r}
library(lme4)
dt <- copy(rtn)
dt <- dt[!is.na(event)]

t <- get_ors(outcome="event", predictor="trt", data=dt, digits = 3)
t

m <- glmer(event ~ trt + (1 | id)
         , data=dt
         , family = binomial(link="logit")
           )


summary(m)

exp(fixef(m))



```

## Bayesian Longitudinal Mixed-Effect Model R::brms

```{r,eval=FALSE}
library(brms)
dt <- copy(rtn)
dt <- dt[!is.na(event)]

m <- brm(event ~ trt + (1 | id)
       , data=dt
       , family = binomial(link=logit)
       , chain=5
       , cores=5
       , warmup=200
       , iter = 400
         )

summary(m)
exp(fixef(m))
ranef(m)

```

```{r,eval=FALSE}

library(rmsb)

n_iter <- 8000
n_warmup <- n_iter / 2
t1 <- Sys.time()
library(rmsb)
dd <- datadist(dt)
options(datadist='dd')
file.remove("discrete_death_blrm.RDS")
m <- blrm(daily_death ~ age_group +
              rcs(day_assessment_icu, 5) +
              days_admit2icu3 +
              sex_2.factor +
              race_ethnicity +
              com_astha +
              com_coronary +
              com_immunosuppresion +
              com_diabetes +
              com_hypertension +
              com_stroke +
              com_renal +
              com_copd +
              com_heartfailure +
              com_obesity +
              cluster(record_id) +
              redcap_data_access_group
        , data=dt
        , iter=n_iter
        , warmup=n_warmup
        , refresh = 500
        , progress = "discrete_death_blrm_progess.log"
        , chain=4
        , cores=4
        , show_messages=TRUE
        , file="discrete_death_blrm.RDS"
          )
t2 <- Sys.time()
print(t2 - t1)
st <- blrmStats(m, ns=n_iter * 4 / 2)
## get concordance probabiilty etc.
saveRDS(st, file="discrete_death_blrmStats.RDS")




m <- readRDS(file="discrete_death_blrm.RDS")
smry <- summary(m)
rNames <- attr(smry, "dimnames")[[1]]
cNames <- attr(smry, "dimnames")[[2]]
smry <- as.data.table(smry)
colnames(smry) <- cNames
smry$coef_name <- rNames

smry[, c(9, 1, 2, 4, 6, 7)] %>%
    kable(caption="Coefficients of the Model (Odds Ratios)"
      , digits=2
      , align="rrrrrrrrrrrrrrrr"
        ) %>% styling()

```


## Bayesian Longitudinal Mixed-Effect Logistic R::rmsb

```{r, eval=FALSE}

t1 <- Sys.time()
library(rmsb)
dd <- datadist(dt)
options(datadist='dd')
file.remove("discrete_death_blrm.RDS")
m <- blrm(event_death ~ age_group +
              rcs(day_assessment_icu, 5) +
              days_admit2icu3 +
              sex_2.factor +
              race_ethnicity +
              com_astha +
              com_coronary +
              com_immunosuppresion +
              com_diabetes +
              com_hypertension +
              com_stroke +
              com_renal +
              com_copd +
              com_heartfailure +
              com_obesity +
              cluster(record_id) +
              redcap_data_access_group
        , data=dt
        , iter=2000
        , warmup=1000
        , refresh = 500
        , progress = "discrete_death_blrm_progess.log"
        , chain=4
        , cores=4
        , show_messages=TRUE
        , file="discrete_death_blrm.RDS"
          )
t2 <- Sys.time()
print(t2 - t1)
st <- blrmStats(m, ns=4000)
## get concordance probabiilty etc.
saveRDS(st, file="discrete_death_blrmStats.RDS")


```
# Multi-state Transition Model

Christopher Jackcon: Multi-state modelling with R: the msm package
https://cran.r-project.org/web/packages/msm/vignettes/msm-manual.pdf


## Basic Setting

```{r}
library(msm)

cav[1:21, ] %>% prt()

statetable.msm(state, PTNUM, data=cav) %>% prt()

Q <- rbind ( c(0, 0.25, 0, 0.25),
            c(0.166, 0, 0.166, 0.166),
            c(0, 0.25, 0, 0.25),
            c(0, 0, 0, 0) )

Q.crude <- crudeinits.msm(state ~ years, PTNUM, data=cav,
                            qmatrix=Q)
```

## Simple bidirectional model

 * The sizes of the confidence intervals for some of the hazard ratios suggest there is no information in the data about the corresponding covariate effects.


```{r}
cav.msm <- msm( state ~ years, subject=PTNUM, data = cav, qmatrix = Q, deathexact = 4)
t <- qmatrix.msm(cav.msm)

t$estimates %>% prt()

cavsex.msm <- msm( state ~ years, subject=PTNUM, data = cav, qmatrix = Q, deathexact = 4, covariates = ~ sex)

qmatrix.msm(cavsex.msm)$estimates %>% prt()

qmatrix.msm(cavsex.msm, covariates=list(sex=0))$estimates %>% prt()

```

## Transition-specific covariate

 * The transition rate from state 1 to state 2, and the rate from state 1 to state 4 are each modelled on sex as a covariate, but no other intensities have covariates on them.
 * The covariate will not apply hazard ratio on the specified transition.


```{r}

cavsex2.msm <- msm( state ~ years
                , subject=PTNUM
                , data = cav
                , qmatrix = Q
                , deathexact = 4
                , covariates = list("1-2" = ~ sex, "1-4" = ~sex)
                  )
qmatrix.msm(cavsex2.msm)$estimates %>% prt()

```

## Constrained covariate effects

 * This contrains the effect of sex to be equal for the progression rates q12, q23, and recovery rates q21, q32.
 * The index on the transition intensities, ordered by rows, could be used as constraint indicators. There are totally 7 intensities and 7 hazard ratios for covariate sex. list(sex=c(1,2,3,4,5,6,7)) means all hazard ratios are distinct, and list(sex=c(1,2,3,4,5,6,1) means the first and 7th hazard ratios are same.


```{r}
cav3.msm <- msm( state ~ years, subject=PTNUM
              , data = cav, qmatrix = Q, deathexact = 4
              , covariates = ~ sex
              , constraint = list(sex=c(1,2,3,4, 5, 6, 1)) )
qmatrix.msm(cav3.msm)$estimates %>% prt()
```

# Competing Risk

```{r, eval = FALSE}
library(cmprsk)

cuminc1 <- cuminc(
  ftime = e2$days_1yr_cm
  , fstatus = e2$status_1yr
  , cencode = "Censored"
  , rho=0)

png(file="",width=12,height=8,units = "in",res=600)
plot(0,0, type="n", xlim=c(0, 365), ylim=c(0, 1)
     , main="Cumulative curves with death as a competing risk",
     xlab= "Days in after Enrollment", ylab= "Cumulative incidence of and Death"
     , col=c("red", "blue", "red", "blue"))
# lines(cuminc1$`1 Censored`$time, cuminc1$`1 Censored`$est, lwd=1,col="black")
lines(cuminc1$`1 Death`$time, cuminc1$`1 Death`$est, lwd=2,col="red")
lines(cuminc1$`1 ARV Pick up`$time, cuminc1$`1 ARV Pick up`$est, lwd=2,col="blue")
legend('topright', c("Event", "Death"), col=c("blue", "red"), lty=2)
dev.off()

```
# Computing Environment


```{r}
sessionInfo()

```
